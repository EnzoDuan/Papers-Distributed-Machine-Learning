# Papers-Distributed-Machine-Learning
This is a repo of distributed machine learning sorted papers, and it will be updated continually.

## Slides

* Distributed Machine Learning: A Brief Overview : [Sildes](https://github.com/EnzoDuan/Papers-Distributed-Machine-Learning/blob/master/podc2018-tutorial-alistarh.pdf)

## Edge Computing

* Distributed Machine Learning on Mobile Devices: A Survey: [Paper](https://github.com/EnzoDuan/Papers-Distributed-Machine-Learning/blob/master/edge_computing/DMLM_survey.pdf)
* Adaptive Federated Learning in Resource Constrained Edge Computing Systems: [Paper](https://github.com/EnzoDuan/Papers-Distributed-Machine-Learning/blob/master/edge_computing/Adaptive_Federated-Learning-in-Resource-Constrained-Edge-Computing-Systems.pdf) / [Abstract](https://github.com/EnzoDuan/Papers-Distributed-Machine-Learning/blob/master/edge_computing/abstract-of-infocom18.pdf)

## Distributed Algorithm

* Deep learning with COTS HPC systems: [Paper](https://github.com/EnzoDuan/Papers-Distributed-Machine-Learning/blob/master/perspective_of_algorithm/Deep_learning_with_COTS_HPC_systems.pdf)
* Large Scale Distributed Deep Networks: [Paper](https://github.com/EnzoDuan/Papers-Distributed-Machine-Learning/blob/master/perspective_of_algorithm/Large_Scale_Distributed_Deep_Networks.pdf)
* More Effective Distributed ML via a Stale Synchronous Parallel Parameter Server: [Paper](https://github.com/EnzoDuan/Papers-Distributed-Machine-Learning/blob/master/perspective_of_algorithm/More_Effective_Distributed_ML_via_a_Stale_Synchronous_Parallel_Parameter_Server.pdf)
* Strategies and Principles of Distributed Machine Learning on Big Data: [Paper](https://github.com/EnzoDuan/Papers-Distributed-Machine-Learning/blob/master/perspective_of_algorithm/Strategies_and_Principles_of_Distributed_Machine_Learning_on_Big_Data.pdf)

## Distributed System

* Petuum: A New Platform for Distributed Machine Learning on Big Data: [Paper](https://github.com/EnzoDuan/Papers-Distributed-Machine-Learning/blob/master/perspective_of_system/petuum.pdf)
* Bosen: Managed Communication and Consistency for Fast Data-Parallel Iterative Analytics: [Paper](https://github.com/EnzoDuan/Papers-Distributed-Machine-Learning/blob/master/perspective_of_system/Bosen.pdf)
* Project Adam: Building an Efficient and Scalable Deep Learning Training System: [Paper](https://github.com/EnzoDuan/Papers-Distributed-Machine-Learning/blob/master/perspective_of_system/Project_Adam_Building_an_Efficient_and_Scalable_Deep_Learning_Training_System.pdf)
* Tiresias: A GPU Cluster Manager for Distributed Deep Learning: [Paper](https://github.com/EnzoDuan/Papers-Distributed-Machine-Learning/blob/master/perspective_of_system/tiresias.pdf)

## Distributed Method of Model

* Asymptotically Exact, Embarrassingly Parallel MCMC: [Paper](https://github.com/EnzoDuan/Papers-Distributed-Machine-Learning/blob/master/perspective_of_model/Asymptotically_Exact_Embarrassingly_Parallel_MCMC.pdf)
* LightLDA: Big Topic Models on Modest Computer Clusters: [Paper](https://github.com/EnzoDuan/Papers-Distributed-Machine-Learning/blob/master/perspective_of_model/LightLDA_Big_Topic_Models_on_Modest_Computer_Clusters.pdf)
* SaberLDA: Sparsity-Aware Learning of Topic Models on GPUs: [Paper](https://github.com/EnzoDuan/Papers-Distributed-Machine-Learning/blob/master/perspective_of_model/SaSaberLDA_Sparsity_Aware_Learning_of_Topic_Models_on_GPUs.pdf)

## Application

